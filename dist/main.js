(()=>{"use strict";var t={m:{},u:t=>t+".js"};t.g=function(){if("object"==typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(t){if("object"==typeof window)return window}}(),t.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{var e;t.g.importScripts&&(e=t.g.location+"");var i=t.g.document;if(!e&&i&&(i.currentScript&&(e=i.currentScript.src),!e)){var o=i.getElementsByTagName("script");o.length&&(e=o[o.length-1].src)}if(!e)throw new Error("Automatic publicPath is not supported in this browser");e=e.replace(/#.*$/,"").replace(/\?.*$/,"").replace(/\/[^\/]+$/,"/"),t.p=e})(),t.b=document.baseURI||self.location.href;const e=class{constructor(t,e,i={x:0,y:0},o){this.domNode=document.querySelector(t),this.position=i,this.loopingPoint=e,this.speed=o}update(t){this.loopingPoint?this.position.x=Math.round((this.position.x+-t*this.speed)%this.loopingPoint):this.position.x=Math.round(this.position.x+-t*this.speed)}setPosition(t){const{x:e,y:i}=t;this.position.x=e,this.position.y=i}render(){this.domNode.style.left=this.position.x,this.domNode.style.top=this.position.y}},i=1/30;!function(){const o=new e(".background",413,void 0,i),n=new e(".ground",500,{x:0,y:272},.06666666666666667),a=new e(".pipe",void 0),s=new e(".bird",{x:150,y:150}),r=new class{constructor(){this.pipe={x:670,height:150},this.pipeSpeed=.06666666666666667}getPipePosition(){return this.pipe}update(t){this.pipe.x=Math.round(this.pipe.x+-t*this.pipeSpeed),this.pipe.x<-50&&this.reset()}getReward(t){return this.isTerminalState(t)?-1e3:1}reset(){this.pipe={x:670,height:150}}isTerminalState(t){const{nextPipeDistanceX:e,nextPipeDistanceY:i,nextDistanceGround:o}=t;return o<=10||e<=10&&i<=10&&!(e<=-75)}getRandomArbitrary(t,e){return Math.random()*(e-t)+t}},l=new class{constructor(t=.95,e=.005,i=.7){this.position={x:150,y:150},this.gravity=45,this.dy=0,this.qValues={},this.discount=t,this.epsilon=e,this.learningRate=i,this.reward=0}update(t){this.dy=this.dy+t/this.gravity,this.position.y=Math.round(this.position.y+this.dy)}getAgentPosition(){return this.position}getBestAction(t){const e=this.getLegalActions(),i=Math.floor(Math.random()*e.length);let o=-1/0,n=null;for(let i in e){const e=this.getQValue(t,i);e>o&&(o=e,n=i)}return Math.random()<this.epsilon?e[i]:n}getQValue(t,e){return this.qValues[t]||(this.qValues[t]=[0,0]),this.qValues[t][e]}updateQValue(t,e,i,o){const n=this.getLegalActions(),a=Math.max(...n.map((t=>this.getQValue(i,t)))),s=this.getQValue(t,e),r=o+this.discount*a-s;this.qValues[t][e]=s+this.learningRate*r}getLegalActions(){return[0,1]}takeAction(t){1===Number(t)&&(this.dy=-2)}reset(){this.position={x:150,y:150},this.dy=0,this.reward=0}getReward(){return this.reward}updateReward(t){this.reward+=t}getQValues(){return this.qValues}},h=document.querySelector(".train-iterations"),c=document.querySelector(".progress"),u=[],d=new Worker(new URL(t.p+t.u(304),t.b));let p,g=0,m=0,y=!0,w=0;function v(){r.reset(),l.reset(),a.setPosition({x:670,y:170}),s.setPosition({x:150,y:150})}function x(t){p=requestAnimationFrame(x),g=t-m,m=t,function(t){const e=r.getPipePosition(),i=l.getAgentPosition(),h=Math.round(e.x-i.x),c=Math.round(e.height-i.y),d=Math.round(n.position.y-i.y),p=l.getBestAction([h,c,d]);l.takeAction(p),l.update(t),r.update(t),o.update(t),n.update(t);const g=r.getPipePosition(),m=l.getAgentPosition(),x=Math.round(g.x-m.x),f=Math.round(g.height-m.y),S=Math.round(n.position.y-m.y),b={currentPipeDistanceX:h,currentPipeDistanceY:c,currentDistanceGround:d},P={nextPipeDistanceX:x,nextPipeDistanceY:f,nextDistanceGround:S},A=r.getReward(b,p,P);l.updateReward(A),y&&l.updateQValue([h,c,d],p,[x,f,S],A);const q=r.isTerminalState(P);q?(u.push({x:u.length+1,y:l.getReward()}),w++,v()):(a.setPosition({x:g.x,y:g.height}),s.setPosition({x:m.x,y:m.y}))}(g),a.render(),o.render(),n.render(),s.render(),document.querySelector(".episode").innerHTML=w}document.querySelector(".train-start-btn").addEventListener("click",(()=>{v(),y=!0,document.title="[TRAINING] Flappy Bird",setTimeout((()=>{c.max=Number(h.value),c.classList.remove("hidden"),d.postMessage({type:"START_TRAINING",parameters:{iterations:Number(h.value)}})}),100)})),document.querySelector(".start-simulation").addEventListener("click",(()=>{v(),y=!1,p=requestAnimationFrame(x)})),document.querySelector(".stop-simulation").addEventListener("click",(()=>{v(),cancelAnimationFrame(p)})),document.querySelector(".show-chart").addEventListener("click",(()=>{d.postMessage({type:"EXPORT_SCORES"})})),document.querySelector(".clear-chart").addEventListener("click",(()=>{document.querySelector("svg").innerHTML=""})),document.querySelector(".epsilon").addEventListener("change",(t=>{l.epsilon=Number(t.target.value)})),document.querySelector(".alpha").addEventListener("change",(t=>{l.learningRate=Number(t.target.value)})),document.querySelector(".gamma").addEventListener("change",(t=>{l.reward=Number(t.target.value)})),tippy("#epsilon",{content:"This parameter represents the exploration vs. exploitation trade-off in the Q-value iteration algorithm. It determines the probability of the agent taking a random action instead of the action with the highest Q-value (i.e., exploration) vs. the action with the highest Q-value (i.e., exploitation). <p>A high value of ε indicates that the agent is more likely to explore, while a low value of ε indicates that the agent is more likely to exploit. Setting ε too high can cause the agent to explore too much and potentially miss the optimal policy, while setting it too low can cause the agent to exploit too much and potentially get stuck in local optima.</p>",allowHTML:!0}),tippy("#alpha",{content:"This parameter represents the learning rate in the Q-value update. It determines how much weight should be given to the new Q-value estimate vs. the previous Q-value estimate. A high value of α indicates that the new Q-value estimate should be given more weight, while a low value of α indicates that the previous Q-value estimate should be given more weight. Setting α too high can cause the algorithm to converge too quickly and potentially miss the optimal policy, while setting it too low can cause the algorithm to converge too slowly.",allowHTML:!0}),tippy("#gamma",{content:"This parameter determines the importance of future rewards in the Q-value update. It is a discount factor that discounts the value of future rewards based on how far away they are in time. In other words, it determines how much weight should be given to immediate rewards vs. future rewards. A high value of γ (e.g., close to 1.0) indicates that future rewards are important, while a low value of γ (e.g., close to 0.0) indicates that only immediate rewards matter.",allowHTML:!0}),d.onmessage=t=>{const{type:e,parameters:i}=t.data;if("END_TRAINING"===e&&(document.title="Flappy Bird",c.value=Number(h.value),d.postMessage({type:"EXPORT_Q_VALUES"})),"Q_VALUES"===e){const{qValues:t}=i;l.qValues=t}if("SCORES"===e){const{scores:t}=i;!function(t){document.querySelector("svg").innerHTML="";const e=d3.select("svg").attr("width",640).attr("height",400).append("g").attr("transform","translate(30, 30)"),i=d3.scaleLinear().domain([0,d3.max(t,(t=>t.x))]).range([0,580]),o=d3.scaleLinear().domain([0,d3.max(t,(t=>t.y))]).range([340,0]),n=d3.scaleLinear().domain([0,d3.max(t,(t=>t.y))]).range(["#0EA5E9","#EF4444"]).interpolate(d3.interpolateHcl);e.selectAll("circle").data(t).enter().append("circle").attr("cx",(t=>i(t.x))).attr("cy",(t=>o(t.y))).attr("r",1).attr("fill",(t=>n(t.y))),e.append("g").attr("transform","translate(0, 340)").call(d3.axisBottom(i)),e.append("g").call(d3.axisLeft(o))}(t)}if("PROGRESS"===e){const{iteration:t}=i;c.value=t}}}()})();